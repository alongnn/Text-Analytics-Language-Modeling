logging:
  LOGGER_NAME: 'root'

fetch_data:
  indices:
    start: 45000
    end: 62000
  save_location: "Data/raw/"

clean_data:
  save_location: "Data/clean/"

create_corpus:
  save_location: "Data/processed/"

gen_training:
  char_nn_training_size: 2000000
  # This is the total character size that you would like to take for training
  # Put -1 if you do not want to limit the number of characters to be taken from the corpus to train
  word_nn_training_size: -1

n_gram:
  gram_count: 4
  model_name: "four_model"
  validation_split: 0.1

char_nn:
  seq_length: 128
  batch_size: 250
  embedding_dim: 128
  rnn_type: "lstm"   #can be "lstm" or "gru"
  rnn_layers: 2
  rnn_units: 256
  dropout: 0.3
  epochs: 2
  validation_split: 0.1
  l2_penalty: 0.0003
  model_name: "char_neural_model1"

w2v_model:
  model_name: "word2vec_300_model1"
  size: 300
  min_count: 100
  workers: 4
  window: 10
  sg: 1 #1 or 0
  hs: 1 #1 or 0

word_nn: 
  w2v_model: "Models/word2vec/word2vec_300_model1.model"
  seq_length: 128
  batch_size: 250
  embedding_dim: 300
  rnn_type: "lstm"   #can be "lstm" or "gru"
  rnn_layers: 2
  rnn_units: 256
  dropout: 0.3
  epochs: 5
  validation_split: 0.1
  l2_penalty: 0.0003
  model_name: "word_neural_model1"
  
char_api:
  model_name: "char_neural_model3"
  seq_length: 128